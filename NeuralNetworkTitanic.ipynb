{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"../input/titanic/train.csv\")","execution_count":81,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":82,"outputs":[{"output_type":"execute_result","execution_count":82,"data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=data","execution_count":83,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.head()","execution_count":84,"outputs":[{"output_type":"execute_result","execution_count":84,"data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=x.drop(['PassengerId','Cabin','Ticket'],axis=1)\nx.head()","execution_count":85,"outputs":[{"output_type":"execute_result","execution_count":85,"data":{"text/plain":"   Survived  Pclass                                               Name  \\\n0         0       3                            Braund, Mr. Owen Harris   \n1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n2         1       3                             Heikkinen, Miss. Laina   \n3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n4         0       3                           Allen, Mr. William Henry   \n\n      Sex   Age  SibSp  Parch     Fare Embarked  \n0    male  22.0      1      0   7.2500        S  \n1  female  38.0      1      0  71.2833        C  \n2  female  26.0      0      0   7.9250        S  \n3  female  35.0      1      0  53.1000        S  \n4    male  35.0      0      0   8.0500        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=x.iloc[:,:1].values","execution_count":86,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":87,"outputs":[{"output_type":"execute_result","execution_count":87,"data":{"text/plain":"array([[0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":88,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le=LabelEncoder()","execution_count":89,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x['Sex']=le.fit_transform(x.Sex)","execution_count":90,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x['Embarked']=le.fit_transform(x.Embarked)","execution_count":91,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"argument must be a string or number","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, encode, check_unknown)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36m_encode_python\u001b[0;34m(values, uniques, encode)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muniques\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'float'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-91-2661d33adaac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Embarked'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbarked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \"\"\"\n\u001b[1;32m    251\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, encode, check_unknown)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"argument must be a string or number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: argument must be a string or number"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=x.drop(['Name'],axis=1)\nx.head()","execution_count":92,"outputs":[{"output_type":"execute_result","execution_count":92,"data":{"text/plain":"   Survived  Pclass  Sex   Age  SibSp  Parch     Fare Embarked\n0         0       3    1  22.0      1      0   7.2500        S\n1         1       1    0  38.0      1      0  71.2833        C\n2         1       3    0  26.0      0      0   7.9250        S\n3         1       1    0  35.0      1      0  53.1000        S\n4         0       3    1  35.0      0      0   8.0500        S","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.iloc[:,7:8]=x.iloc[:,7:8].apply(lambda x:x.fillna(x.value_counts().index[0]))","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.tail(25)","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"     Survived  Pclass  Sex   Age  SibSp  Parch     Fare Embarked\n866         1       2    0  27.0      1      0  13.8583        S\n867         0       1    1  31.0      0      0  50.4958        S\n868         0       3    1   NaN      0      0   9.5000        S\n869         1       3    1   4.0      1      1  11.1333        S\n870         0       3    1  26.0      0      0   7.8958        S\n871         1       1    0  47.0      1      1  52.5542        S\n872         0       1    1  33.0      0      0   5.0000        S\n873         0       3    1  47.0      0      0   9.0000        S\n874         1       2    0  28.0      1      0  24.0000        S\n875         1       3    0  15.0      0      0   7.2250        S\n876         0       3    1  20.0      0      0   9.8458        S\n877         0       3    1  19.0      0      0   7.8958        S\n878         0       3    1   NaN      0      0   7.8958        S\n879         1       1    0  56.0      0      1  83.1583        S\n880         1       2    0  25.0      0      1  26.0000        S\n881         0       3    1  33.0      0      0   7.8958        S\n882         0       3    0  22.0      0      0  10.5167        S\n883         0       2    1  28.0      0      0  10.5000        S\n884         0       3    1  25.0      0      0   7.0500        S\n885         0       3    0  39.0      0      5  29.1250        S\n886         0       2    1  27.0      0      0  13.0000        S\n887         1       1    0  19.0      0      0  30.0000        S\n888         0       3    0   NaN      1      2  23.4500        S\n889         1       1    1  26.0      0      0  30.0000        S\n890         0       3    1  32.0      0      0   7.7500        S","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>866</th>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>13.8583</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>867</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>31.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>50.4958</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>868</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9.5000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>869</th>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>11.1333</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>870</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.8958</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>871</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>52.5542</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>872</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5.0000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>873</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>47.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9.0000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>874</th>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>24.0000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>875</th>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>15.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.2250</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>876</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>20.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9.8458</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>877</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.8958</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>878</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.8958</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>879</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>56.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>83.1583</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>880</th>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>25.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>26.0000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>881</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>33.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.8958</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>882</th>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>22.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10.5167</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>883</th>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10.5000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>884</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>25.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.0500</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>885</th>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>39.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>29.1250</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.0000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2</td>\n      <td>23.4500</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7500</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x['Embarked']=le.fit_transform(x.Embarked)","execution_count":94,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.head()","execution_count":95,"outputs":[{"output_type":"execute_result","execution_count":95,"data":{"text/plain":"   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n0         0       3    1  22.0      1      0   7.2500         2\n1         1       1    0  38.0      1      0  71.2833         0\n2         1       3    0  26.0      0      0   7.9250         2\n3         1       1    0  35.0      1      0  53.1000         2\n4         0       3    1  35.0      0      0   8.0500         2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_x=x.iloc[:,1:8].values","execution_count":96,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_x","execution_count":97,"outputs":[{"output_type":"execute_result","execution_count":97,"data":{"text/plain":"array([[ 3.    ,  1.    , 22.    , ...,  0.    ,  7.25  ,  2.    ],\n       [ 1.    ,  0.    , 38.    , ...,  0.    , 71.2833,  0.    ],\n       [ 3.    ,  0.    , 26.    , ...,  0.    ,  7.925 ,  2.    ],\n       ...,\n       [ 3.    ,  0.    ,     nan, ...,  2.    , 23.45  ,  2.    ],\n       [ 1.    ,  1.    , 26.    , ...,  0.    , 30.    ,  0.    ],\n       [ 3.    ,  1.    , 32.    , ...,  0.    ,  7.75  ,  1.    ]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer","execution_count":98,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=SimpleImputer(missing_values=np.nan,strategy='mean',verbose=0)","execution_count":99,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=s.fit(df_x[:,:])","execution_count":100,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_x[:,:]=s.transform(df_x[:,:])","execution_count":101,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_x","execution_count":102,"outputs":[{"output_type":"execute_result","execution_count":102,"data":{"text/plain":"array([[ 3.        ,  1.        , 22.        , ...,  0.        ,\n         7.25      ,  2.        ],\n       [ 1.        ,  0.        , 38.        , ...,  0.        ,\n        71.2833    ,  0.        ],\n       [ 3.        ,  0.        , 26.        , ...,  0.        ,\n         7.925     ,  2.        ],\n       ...,\n       [ 3.        ,  0.        , 29.69911765, ...,  2.        ,\n        23.45      ,  2.        ],\n       [ 1.        ,  1.        , 26.        , ...,  0.        ,\n        30.        ,  0.        ],\n       [ 3.        ,  1.        , 32.        , ...,  0.        ,\n         7.75      ,  1.        ]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":103,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(df_x,y,test_size=0.25,random_state=0)","execution_count":104,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":105,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras","execution_count":106,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential","execution_count":107,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense","execution_count":108,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier=Sequential()","execution_count":109,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.add(Dense(output_dim=4,init='uniform',activation='relu',input_dim=7))","execution_count":110,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=7, units=4, kernel_initializer=\"uniform\")`\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.add(Dense(output_dim=4,init='uniform',activation='relu'))","execution_count":111,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=4, kernel_initializer=\"uniform\")`\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.add(Dense(output_dim=1,init='uniform',activation='sigmoid'))","execution_count":112,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","execution_count":113,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":114,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc=StandardScaler()","execution_count":115,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train=sc.fit_transform(x_train)","execution_count":116,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test=sc.transform(x_test)","execution_count":117,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":118,"outputs":[{"output_type":"execute_result","execution_count":118,"data":{"text/plain":"array([[0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(x_train,y_train,batch_size=20,nb_epoch=1000)","execution_count":119,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"},{"output_type":"stream","text":"Epoch 1/1000\n668/668 [==============================] - 0s 323us/step - loss: 0.6919 - accuracy: 0.6093\nEpoch 2/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.6883 - accuracy: 0.6138\nEpoch 3/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.6828 - accuracy: 0.6138\nEpoch 4/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.6735 - accuracy: 0.6138\nEpoch 5/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.6590 - accuracy: 0.6153\nEpoch 6/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.6387 - accuracy: 0.7036\nEpoch 7/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.6135 - accuracy: 0.7754\nEpoch 8/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.5853 - accuracy: 0.8009\nEpoch 9/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.5578 - accuracy: 0.7979\nEpoch 10/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.5328 - accuracy: 0.7994\nEpoch 11/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.5125 - accuracy: 0.7979\nEpoch 12/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4969 - accuracy: 0.7964\nEpoch 13/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4863 - accuracy: 0.7964\nEpoch 14/1000\n668/668 [==============================] - 0s 61us/step - loss: 0.4782 - accuracy: 0.7889\nEpoch 15/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4724 - accuracy: 0.7889\nEpoch 16/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4686 - accuracy: 0.7889\nEpoch 17/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4659 - accuracy: 0.7889\nEpoch 18/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4635 - accuracy: 0.7889\nEpoch 19/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4619 - accuracy: 0.7904\nEpoch 20/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4602 - accuracy: 0.7964\nEpoch 21/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4584 - accuracy: 0.7964\nEpoch 22/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4571 - accuracy: 0.8009\nEpoch 23/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4553 - accuracy: 0.7979\nEpoch 24/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4543 - accuracy: 0.8009\nEpoch 25/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4531 - accuracy: 0.8024\nEpoch 26/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4522 - accuracy: 0.8009\nEpoch 27/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4513 - accuracy: 0.8009\nEpoch 28/1000\n668/668 [==============================] - 0s 61us/step - loss: 0.4507 - accuracy: 0.8009\nEpoch 29/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4498 - accuracy: 0.8009\nEpoch 30/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4493 - accuracy: 0.8024\nEpoch 31/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4481 - accuracy: 0.8009\nEpoch 32/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4475 - accuracy: 0.8024\nEpoch 33/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4466 - accuracy: 0.8009\nEpoch 34/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4468 - accuracy: 0.8009\nEpoch 35/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4457 - accuracy: 0.8024\nEpoch 36/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4450 - accuracy: 0.8039\nEpoch 37/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4445 - accuracy: 0.8039\nEpoch 38/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4441 - accuracy: 0.8039\nEpoch 39/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4438 - accuracy: 0.8039\nEpoch 40/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4434 - accuracy: 0.8039\nEpoch 41/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4428 - accuracy: 0.8039\nEpoch 42/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4422 - accuracy: 0.8039\nEpoch 43/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4421 - accuracy: 0.8024\nEpoch 44/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4416 - accuracy: 0.8054\nEpoch 45/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4411 - accuracy: 0.8039\nEpoch 46/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4410 - accuracy: 0.8054\nEpoch 47/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4402 - accuracy: 0.8054\nEpoch 48/1000\n668/668 [==============================] - 0s 61us/step - loss: 0.4401 - accuracy: 0.8039\nEpoch 49/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4396 - accuracy: 0.8054\nEpoch 50/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4395 - accuracy: 0.8054\nEpoch 51/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4394 - accuracy: 0.8039\nEpoch 52/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4392 - accuracy: 0.8054\nEpoch 53/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4386 - accuracy: 0.8024\nEpoch 54/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4386 - accuracy: 0.8054\nEpoch 55/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4383 - accuracy: 0.8039\nEpoch 56/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4383 - accuracy: 0.8069\nEpoch 57/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4380 - accuracy: 0.8039\nEpoch 58/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4377 - accuracy: 0.8024\nEpoch 59/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4373 - accuracy: 0.8069\nEpoch 60/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4372 - accuracy: 0.8069\nEpoch 61/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4371 - accuracy: 0.8084\nEpoch 62/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4368 - accuracy: 0.8084\nEpoch 63/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4372 - accuracy: 0.8039\nEpoch 64/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4363 - accuracy: 0.8054\nEpoch 65/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4365 - accuracy: 0.8084\nEpoch 66/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4363 - accuracy: 0.8084\nEpoch 67/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4357 - accuracy: 0.8084\nEpoch 68/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4357 - accuracy: 0.8084\nEpoch 69/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4355 - accuracy: 0.8084\nEpoch 70/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4354 - accuracy: 0.8084\nEpoch 71/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4355 - accuracy: 0.8069\nEpoch 72/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4354 - accuracy: 0.8054\nEpoch 73/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4344 - accuracy: 0.8084\nEpoch 74/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4345 - accuracy: 0.8069\nEpoch 75/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4345 - accuracy: 0.8084\nEpoch 76/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4341 - accuracy: 0.8099\nEpoch 77/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4340 - accuracy: 0.8084\nEpoch 78/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4339 - accuracy: 0.8084\nEpoch 79/1000\n","name":"stdout"},{"output_type":"stream","text":"668/668 [==============================] - 0s 67us/step - loss: 0.4336 - accuracy: 0.8069\nEpoch 80/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4334 - accuracy: 0.8054\nEpoch 81/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4332 - accuracy: 0.8054\nEpoch 82/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4334 - accuracy: 0.8054\nEpoch 83/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4331 - accuracy: 0.8054\nEpoch 84/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4336 - accuracy: 0.8069\nEpoch 85/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4328 - accuracy: 0.8084\nEpoch 86/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4328 - accuracy: 0.8054\nEpoch 87/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4327 - accuracy: 0.8069\nEpoch 88/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4327 - accuracy: 0.8069\nEpoch 89/1000\n668/668 [==============================] - 0s 61us/step - loss: 0.4326 - accuracy: 0.8039\nEpoch 90/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4324 - accuracy: 0.8024\nEpoch 91/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4314 - accuracy: 0.8069\nEpoch 92/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4308 - accuracy: 0.8069\nEpoch 93/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4306 - accuracy: 0.8129\nEpoch 94/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4302 - accuracy: 0.8084\nEpoch 95/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4300 - accuracy: 0.8099\nEpoch 96/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4295 - accuracy: 0.8144\nEpoch 97/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4292 - accuracy: 0.8144\nEpoch 98/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4291 - accuracy: 0.8189\nEpoch 99/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4291 - accuracy: 0.8144\nEpoch 100/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4287 - accuracy: 0.8189\nEpoch 101/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4282 - accuracy: 0.8174\nEpoch 102/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4279 - accuracy: 0.8159\nEpoch 103/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4279 - accuracy: 0.8159\nEpoch 104/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4273 - accuracy: 0.8174\nEpoch 105/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4275 - accuracy: 0.8084\nEpoch 106/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4270 - accuracy: 0.8174\nEpoch 107/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4264 - accuracy: 0.8204\nEpoch 108/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4264 - accuracy: 0.8189\nEpoch 109/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4262 - accuracy: 0.8159\nEpoch 110/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4260 - accuracy: 0.8204\nEpoch 111/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4257 - accuracy: 0.8174\nEpoch 112/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4255 - accuracy: 0.8174\nEpoch 113/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4256 - accuracy: 0.8174\nEpoch 114/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4252 - accuracy: 0.8174\nEpoch 115/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4247 - accuracy: 0.8189\nEpoch 116/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4247 - accuracy: 0.8159\nEpoch 117/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4246 - accuracy: 0.8159\nEpoch 118/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4242 - accuracy: 0.8174\nEpoch 119/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4244 - accuracy: 0.8159\nEpoch 120/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4237 - accuracy: 0.8234\nEpoch 121/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4233 - accuracy: 0.8204\nEpoch 122/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4231 - accuracy: 0.8159\nEpoch 123/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4228 - accuracy: 0.8144\nEpoch 124/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4230 - accuracy: 0.8174\nEpoch 125/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4222 - accuracy: 0.8159\nEpoch 126/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4222 - accuracy: 0.8174\nEpoch 127/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4214 - accuracy: 0.8174\nEpoch 128/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4216 - accuracy: 0.8174\nEpoch 129/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4213 - accuracy: 0.8204\nEpoch 130/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4207 - accuracy: 0.8219\nEpoch 131/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4204 - accuracy: 0.8219\nEpoch 132/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4203 - accuracy: 0.8219\nEpoch 133/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4201 - accuracy: 0.8204\nEpoch 134/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4198 - accuracy: 0.8234\nEpoch 135/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4196 - accuracy: 0.8249\nEpoch 136/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4198 - accuracy: 0.8219\nEpoch 137/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4193 - accuracy: 0.8219\nEpoch 138/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4192 - accuracy: 0.8219\nEpoch 139/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4191 - accuracy: 0.8249\nEpoch 140/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4188 - accuracy: 0.8219\nEpoch 141/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4185 - accuracy: 0.8234\nEpoch 142/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4187 - accuracy: 0.8219\nEpoch 143/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4185 - accuracy: 0.8234\nEpoch 144/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4186 - accuracy: 0.8219\nEpoch 145/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4185 - accuracy: 0.8234\nEpoch 146/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4177 - accuracy: 0.8234\nEpoch 147/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4180 - accuracy: 0.8249\nEpoch 148/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4181 - accuracy: 0.8263\nEpoch 149/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4172 - accuracy: 0.8278\nEpoch 150/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4177 - accuracy: 0.8249\nEpoch 151/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4173 - accuracy: 0.8263\nEpoch 152/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4169 - accuracy: 0.8234\nEpoch 153/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4167 - accuracy: 0.8249\nEpoch 154/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4168 - accuracy: 0.8249\nEpoch 155/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4167 - accuracy: 0.8234\nEpoch 156/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4166 - accuracy: 0.8234\nEpoch 157/1000\n","name":"stdout"},{"output_type":"stream","text":"668/668 [==============================] - 0s 64us/step - loss: 0.4165 - accuracy: 0.8234\nEpoch 158/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4165 - accuracy: 0.8249\nEpoch 159/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4167 - accuracy: 0.8219\nEpoch 160/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4162 - accuracy: 0.8263\nEpoch 161/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4160 - accuracy: 0.8263\nEpoch 162/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4160 - accuracy: 0.8263\nEpoch 163/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4159 - accuracy: 0.8263\nEpoch 164/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4160 - accuracy: 0.8249\nEpoch 165/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4159 - accuracy: 0.8249\nEpoch 166/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4156 - accuracy: 0.8249\nEpoch 167/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4156 - accuracy: 0.8263\nEpoch 168/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4156 - accuracy: 0.8293\nEpoch 169/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4153 - accuracy: 0.8278\nEpoch 170/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4155 - accuracy: 0.8249\nEpoch 171/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4155 - accuracy: 0.8249\nEpoch 172/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4151 - accuracy: 0.8263\nEpoch 173/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4156 - accuracy: 0.8249\nEpoch 174/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4157 - accuracy: 0.8278\nEpoch 175/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4150 - accuracy: 0.8278\nEpoch 176/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4150 - accuracy: 0.8263\nEpoch 177/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4154 - accuracy: 0.8278\nEpoch 178/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4157 - accuracy: 0.8263\nEpoch 179/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4154 - accuracy: 0.8263\nEpoch 180/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4148 - accuracy: 0.8263\nEpoch 181/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4149 - accuracy: 0.8308\nEpoch 182/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4154 - accuracy: 0.8249\nEpoch 183/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4146 - accuracy: 0.8278\nEpoch 184/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4144 - accuracy: 0.8263\nEpoch 185/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4149 - accuracy: 0.8249\nEpoch 186/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4142 - accuracy: 0.8278\nEpoch 187/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4143 - accuracy: 0.8308\nEpoch 188/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4146 - accuracy: 0.8263\nEpoch 189/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4141 - accuracy: 0.8278\nEpoch 190/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4141 - accuracy: 0.8293\nEpoch 191/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4142 - accuracy: 0.8308\nEpoch 192/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4140 - accuracy: 0.8323\nEpoch 193/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4142 - accuracy: 0.8323\nEpoch 194/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4139 - accuracy: 0.8323\nEpoch 195/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4139 - accuracy: 0.8293\nEpoch 196/1000\n668/668 [==============================] - 0s 85us/step - loss: 0.4140 - accuracy: 0.8278\nEpoch 197/1000\n668/668 [==============================] - 0s 75us/step - loss: 0.4138 - accuracy: 0.8278\nEpoch 198/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4139 - accuracy: 0.8308\nEpoch 199/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4140 - accuracy: 0.8249\nEpoch 200/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4136 - accuracy: 0.8323\nEpoch 201/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4138 - accuracy: 0.8308\nEpoch 202/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4138 - accuracy: 0.8338\nEpoch 203/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4138 - accuracy: 0.8353\nEpoch 204/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4135 - accuracy: 0.8308\nEpoch 205/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4133 - accuracy: 0.8308\nEpoch 206/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4135 - accuracy: 0.8338\nEpoch 207/1000\n668/668 [==============================] - 0s 75us/step - loss: 0.4139 - accuracy: 0.8323\nEpoch 208/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4133 - accuracy: 0.8368\nEpoch 209/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4132 - accuracy: 0.8338\nEpoch 210/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4135 - accuracy: 0.8368\nEpoch 211/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4131 - accuracy: 0.8308\nEpoch 212/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4131 - accuracy: 0.8353\nEpoch 213/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4129 - accuracy: 0.8353\nEpoch 214/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4129 - accuracy: 0.8353\nEpoch 215/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4132 - accuracy: 0.8323\nEpoch 216/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4133 - accuracy: 0.8323\nEpoch 217/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4130 - accuracy: 0.8353\nEpoch 218/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4128 - accuracy: 0.8383\nEpoch 219/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4130 - accuracy: 0.8353\nEpoch 220/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4129 - accuracy: 0.8308\nEpoch 221/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4129 - accuracy: 0.8353\nEpoch 222/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4125 - accuracy: 0.8293\nEpoch 223/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4128 - accuracy: 0.8323\nEpoch 224/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4125 - accuracy: 0.8308\nEpoch 225/1000\n668/668 [==============================] - 0s 61us/step - loss: 0.4126 - accuracy: 0.8353\nEpoch 226/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4124 - accuracy: 0.8353\nEpoch 227/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4126 - accuracy: 0.8338\nEpoch 228/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4125 - accuracy: 0.8323\nEpoch 229/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4122 - accuracy: 0.8323\nEpoch 230/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4127 - accuracy: 0.8323\nEpoch 231/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4123 - accuracy: 0.8308\nEpoch 232/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4122 - accuracy: 0.8353\nEpoch 233/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4122 - accuracy: 0.8383\nEpoch 234/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4123 - accuracy: 0.8398\nEpoch 235/1000\n","name":"stdout"},{"output_type":"stream","text":"668/668 [==============================] - 0s 65us/step - loss: 0.4129 - accuracy: 0.8338\nEpoch 236/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4123 - accuracy: 0.8353\nEpoch 237/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4119 - accuracy: 0.8338\nEpoch 238/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4124 - accuracy: 0.8398\nEpoch 239/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4120 - accuracy: 0.8368\nEpoch 240/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4119 - accuracy: 0.8368\nEpoch 241/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4123 - accuracy: 0.8353\nEpoch 242/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4125 - accuracy: 0.8368\nEpoch 243/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4122 - accuracy: 0.8353\nEpoch 244/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4124 - accuracy: 0.8383\nEpoch 245/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4117 - accuracy: 0.8368\nEpoch 246/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4121 - accuracy: 0.8353\nEpoch 247/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4118 - accuracy: 0.8353\nEpoch 248/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4120 - accuracy: 0.8368\nEpoch 249/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4120 - accuracy: 0.8338\nEpoch 250/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4121 - accuracy: 0.8353\nEpoch 251/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4118 - accuracy: 0.8368\nEpoch 252/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4116 - accuracy: 0.8353\nEpoch 253/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4124 - accuracy: 0.8353\nEpoch 254/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4117 - accuracy: 0.8353\nEpoch 255/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4116 - accuracy: 0.8368\nEpoch 256/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4120 - accuracy: 0.8383\nEpoch 257/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4115 - accuracy: 0.8368\nEpoch 258/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4123 - accuracy: 0.8353\nEpoch 259/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4115 - accuracy: 0.8353\nEpoch 260/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4116 - accuracy: 0.8383\nEpoch 261/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4117 - accuracy: 0.8353\nEpoch 262/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4119 - accuracy: 0.8353\nEpoch 263/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4119 - accuracy: 0.8368\nEpoch 264/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4116 - accuracy: 0.8383\nEpoch 265/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4117 - accuracy: 0.8383\nEpoch 266/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4114 - accuracy: 0.8383\nEpoch 267/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4114 - accuracy: 0.8368\nEpoch 268/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4113 - accuracy: 0.8383\nEpoch 269/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4115 - accuracy: 0.8353\nEpoch 270/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4111 - accuracy: 0.8383\nEpoch 271/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4112 - accuracy: 0.8383\nEpoch 272/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4113 - accuracy: 0.8353\nEpoch 273/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4112 - accuracy: 0.8383\nEpoch 274/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4107 - accuracy: 0.8368\nEpoch 275/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4106 - accuracy: 0.8353\nEpoch 276/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4107 - accuracy: 0.8383\nEpoch 277/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4103 - accuracy: 0.8383\nEpoch 278/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4101 - accuracy: 0.8383\nEpoch 279/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4100 - accuracy: 0.8383\nEpoch 280/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4098 - accuracy: 0.8398\nEpoch 281/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4100 - accuracy: 0.8398\nEpoch 282/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4097 - accuracy: 0.8398\nEpoch 283/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4094 - accuracy: 0.8398\nEpoch 284/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4098 - accuracy: 0.8398\nEpoch 285/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4094 - accuracy: 0.8398\nEpoch 286/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4094 - accuracy: 0.8413\nEpoch 287/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4093 - accuracy: 0.8413\nEpoch 288/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.4091 - accuracy: 0.8353\nEpoch 289/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4094 - accuracy: 0.8353\nEpoch 290/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4086 - accuracy: 0.8413\nEpoch 291/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4086 - accuracy: 0.8413\nEpoch 292/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4086 - accuracy: 0.8383\nEpoch 293/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4097 - accuracy: 0.8413\nEpoch 294/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4081 - accuracy: 0.8413\nEpoch 295/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4084 - accuracy: 0.8443\nEpoch 296/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4085 - accuracy: 0.8413\nEpoch 297/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4081 - accuracy: 0.8413\nEpoch 298/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4083 - accuracy: 0.8413\nEpoch 299/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4080 - accuracy: 0.8428\nEpoch 300/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4083 - accuracy: 0.8443\nEpoch 301/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4077 - accuracy: 0.8443\nEpoch 302/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4080 - accuracy: 0.8413\nEpoch 303/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4075 - accuracy: 0.8428\nEpoch 304/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4079 - accuracy: 0.8413\nEpoch 305/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4077 - accuracy: 0.8413\nEpoch 306/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4080 - accuracy: 0.8413\nEpoch 307/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4077 - accuracy: 0.8428\nEpoch 308/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4076 - accuracy: 0.8413\nEpoch 309/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4079 - accuracy: 0.8413\nEpoch 310/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4075 - accuracy: 0.8428\nEpoch 311/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4078 - accuracy: 0.8413\nEpoch 312/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4077 - accuracy: 0.8413\nEpoch 313/1000\n","name":"stdout"},{"output_type":"stream","text":"668/668 [==============================] - 0s 69us/step - loss: 0.4076 - accuracy: 0.8428\nEpoch 314/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4074 - accuracy: 0.8413\nEpoch 315/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4078 - accuracy: 0.8398\nEpoch 316/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4075 - accuracy: 0.8413\nEpoch 317/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4076 - accuracy: 0.8413\nEpoch 318/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4076 - accuracy: 0.8428\nEpoch 319/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4072 - accuracy: 0.8428\nEpoch 320/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4074 - accuracy: 0.8413\nEpoch 321/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4072 - accuracy: 0.8398\nEpoch 322/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4074 - accuracy: 0.8428\nEpoch 323/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4074 - accuracy: 0.8428\nEpoch 324/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4074 - accuracy: 0.8428\nEpoch 325/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4072 - accuracy: 0.8413\nEpoch 326/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4074 - accuracy: 0.8428\nEpoch 327/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4071 - accuracy: 0.8443\nEpoch 328/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4069 - accuracy: 0.8428\nEpoch 329/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4072 - accuracy: 0.8413\nEpoch 330/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4070 - accuracy: 0.8428\nEpoch 331/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4072 - accuracy: 0.8398\nEpoch 332/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4071 - accuracy: 0.8413\nEpoch 333/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4070 - accuracy: 0.8413\nEpoch 334/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4071 - accuracy: 0.8428\nEpoch 335/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4066 - accuracy: 0.8428\nEpoch 336/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4077 - accuracy: 0.8398\nEpoch 337/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4071 - accuracy: 0.8413\nEpoch 338/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4070 - accuracy: 0.8428\nEpoch 339/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4067 - accuracy: 0.8428\nEpoch 340/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4070 - accuracy: 0.8413\nEpoch 341/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4064 - accuracy: 0.8413\nEpoch 342/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4075 - accuracy: 0.8443\nEpoch 343/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4068 - accuracy: 0.8443\nEpoch 344/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4068 - accuracy: 0.8413\nEpoch 345/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4071 - accuracy: 0.8398\nEpoch 346/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4073 - accuracy: 0.8413\nEpoch 347/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4066 - accuracy: 0.8428\nEpoch 348/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4065 - accuracy: 0.8443\nEpoch 349/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4066 - accuracy: 0.8443\nEpoch 350/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4065 - accuracy: 0.8428\nEpoch 351/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4068 - accuracy: 0.8443\nEpoch 352/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4065 - accuracy: 0.8428\nEpoch 353/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4069 - accuracy: 0.8443\nEpoch 354/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4063 - accuracy: 0.8443\nEpoch 355/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4063 - accuracy: 0.8428\nEpoch 356/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4066 - accuracy: 0.8443\nEpoch 357/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4069 - accuracy: 0.8413\nEpoch 358/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4063 - accuracy: 0.8428\nEpoch 359/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4063 - accuracy: 0.8428\nEpoch 360/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4065 - accuracy: 0.8413\nEpoch 361/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4062 - accuracy: 0.8428\nEpoch 362/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4063 - accuracy: 0.8428\nEpoch 363/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4068 - accuracy: 0.8428\nEpoch 364/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4063 - accuracy: 0.8428\nEpoch 365/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4062 - accuracy: 0.8443\nEpoch 366/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4061 - accuracy: 0.8443\nEpoch 367/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4061 - accuracy: 0.8443\nEpoch 368/1000\n668/668 [==============================] - 0s 61us/step - loss: 0.4063 - accuracy: 0.8443\nEpoch 369/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4066 - accuracy: 0.8443\nEpoch 370/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4064 - accuracy: 0.8413\nEpoch 371/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4064 - accuracy: 0.8428\nEpoch 372/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4063 - accuracy: 0.8443\nEpoch 373/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4062 - accuracy: 0.8443\nEpoch 374/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4064 - accuracy: 0.8443\nEpoch 375/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4062 - accuracy: 0.8428\nEpoch 376/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4064 - accuracy: 0.8443\nEpoch 377/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4060 - accuracy: 0.8428\nEpoch 378/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4061 - accuracy: 0.8428\nEpoch 379/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4061 - accuracy: 0.8428\nEpoch 380/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4059 - accuracy: 0.8443\nEpoch 381/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4061 - accuracy: 0.8443\nEpoch 382/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4060 - accuracy: 0.8428\nEpoch 383/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4060 - accuracy: 0.8443\nEpoch 384/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4058 - accuracy: 0.8443\nEpoch 385/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4059 - accuracy: 0.8413\nEpoch 386/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4060 - accuracy: 0.8428\nEpoch 387/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4058 - accuracy: 0.8443\nEpoch 388/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4060 - accuracy: 0.8443\nEpoch 389/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4061 - accuracy: 0.8458\nEpoch 390/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4058 - accuracy: 0.8443\nEpoch 391/1000\n","name":"stdout"},{"output_type":"stream","text":"668/668 [==============================] - 0s 65us/step - loss: 0.4056 - accuracy: 0.8443\nEpoch 392/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4061 - accuracy: 0.8428\nEpoch 393/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4057 - accuracy: 0.8428\nEpoch 394/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4059 - accuracy: 0.8428\nEpoch 395/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4059 - accuracy: 0.8413\nEpoch 396/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4057 - accuracy: 0.8443\nEpoch 397/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.4060 - accuracy: 0.8443\nEpoch 398/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4057 - accuracy: 0.8398\nEpoch 399/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4058 - accuracy: 0.8428\nEpoch 400/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4063 - accuracy: 0.8428\nEpoch 401/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4057 - accuracy: 0.8428\nEpoch 402/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4065 - accuracy: 0.8443\nEpoch 403/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.4058 - accuracy: 0.8443\nEpoch 404/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4057 - accuracy: 0.8443\nEpoch 405/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4059 - accuracy: 0.8443\nEpoch 406/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4058 - accuracy: 0.8443\nEpoch 407/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4053 - accuracy: 0.8428\nEpoch 408/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4058 - accuracy: 0.8428\nEpoch 409/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4061 - accuracy: 0.8458\nEpoch 410/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4055 - accuracy: 0.8443\nEpoch 411/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4054 - accuracy: 0.8443\nEpoch 412/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4054 - accuracy: 0.8428\nEpoch 413/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4053 - accuracy: 0.8428\nEpoch 414/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4054 - accuracy: 0.8428\nEpoch 415/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4056 - accuracy: 0.8413\nEpoch 416/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.4054 - accuracy: 0.8428\nEpoch 417/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4054 - accuracy: 0.8443\nEpoch 418/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4055 - accuracy: 0.8413\nEpoch 419/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4054 - accuracy: 0.8443\nEpoch 420/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4059 - accuracy: 0.8398\nEpoch 421/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4053 - accuracy: 0.8428\nEpoch 422/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4052 - accuracy: 0.8443\nEpoch 423/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4052 - accuracy: 0.8443\nEpoch 424/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4051 - accuracy: 0.8443\nEpoch 425/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4051 - accuracy: 0.8443\nEpoch 426/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4054 - accuracy: 0.8413\nEpoch 427/1000\n668/668 [==============================] - 0s 78us/step - loss: 0.4055 - accuracy: 0.8443\nEpoch 428/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4049 - accuracy: 0.8443\nEpoch 429/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4049 - accuracy: 0.8443\nEpoch 430/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4051 - accuracy: 0.8428\nEpoch 431/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4052 - accuracy: 0.8443\nEpoch 432/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4052 - accuracy: 0.8443\nEpoch 433/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.4046 - accuracy: 0.8458\nEpoch 434/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4047 - accuracy: 0.8458\nEpoch 435/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4049 - accuracy: 0.8458\nEpoch 436/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4049 - accuracy: 0.8428\nEpoch 437/1000\n668/668 [==============================] - 0s 75us/step - loss: 0.4048 - accuracy: 0.8443\nEpoch 438/1000\n668/668 [==============================] - 0s 87us/step - loss: 0.4047 - accuracy: 0.8458\nEpoch 439/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4049 - accuracy: 0.8428\nEpoch 440/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.4049 - accuracy: 0.8413\nEpoch 441/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.4048 - accuracy: 0.8398\nEpoch 442/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4047 - accuracy: 0.8413\nEpoch 443/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4043 - accuracy: 0.8428\nEpoch 444/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4050 - accuracy: 0.8458\nEpoch 445/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4049 - accuracy: 0.8413\nEpoch 446/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4042 - accuracy: 0.8428\nEpoch 447/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4045 - accuracy: 0.8413\nEpoch 448/1000\n668/668 [==============================] - 0s 74us/step - loss: 0.4044 - accuracy: 0.8443\nEpoch 449/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4043 - accuracy: 0.8458\nEpoch 450/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4042 - accuracy: 0.8413\nEpoch 451/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4043 - accuracy: 0.8443\nEpoch 452/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.4040 - accuracy: 0.8428\nEpoch 453/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.4041 - accuracy: 0.8428\nEpoch 454/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4041 - accuracy: 0.8413\nEpoch 455/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.4040 - accuracy: 0.8458\nEpoch 456/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.4040 - accuracy: 0.8458\nEpoch 457/1000\n668/668 [==============================] - 0s 74us/step - loss: 0.4038 - accuracy: 0.8458\nEpoch 458/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4036 - accuracy: 0.8443\nEpoch 459/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4033 - accuracy: 0.8443\nEpoch 460/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4034 - accuracy: 0.8443\nEpoch 461/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4033 - accuracy: 0.8428\nEpoch 462/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4031 - accuracy: 0.8443\nEpoch 463/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4032 - accuracy: 0.8413\nEpoch 464/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4036 - accuracy: 0.8383\nEpoch 465/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4028 - accuracy: 0.8428\nEpoch 466/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4029 - accuracy: 0.8428\nEpoch 467/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4028 - accuracy: 0.8443\nEpoch 468/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4030 - accuracy: 0.8443\nEpoch 469/1000\n","name":"stdout"},{"output_type":"stream","text":"668/668 [==============================] - 0s 73us/step - loss: 0.4026 - accuracy: 0.8458\nEpoch 470/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4023 - accuracy: 0.8413\nEpoch 471/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.4025 - accuracy: 0.8443\nEpoch 472/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.4026 - accuracy: 0.8413\nEpoch 473/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4020 - accuracy: 0.8443\nEpoch 474/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4020 - accuracy: 0.8428\nEpoch 475/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.4018 - accuracy: 0.8413\nEpoch 476/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4019 - accuracy: 0.8443\nEpoch 477/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4014 - accuracy: 0.8413\nEpoch 478/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4015 - accuracy: 0.8428\nEpoch 479/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4014 - accuracy: 0.8413\nEpoch 480/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4011 - accuracy: 0.8413\nEpoch 481/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4015 - accuracy: 0.8458\nEpoch 482/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4012 - accuracy: 0.8428\nEpoch 483/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4009 - accuracy: 0.8428\nEpoch 484/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4012 - accuracy: 0.8428\nEpoch 485/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4014 - accuracy: 0.8398\nEpoch 486/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4011 - accuracy: 0.8428\nEpoch 487/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4008 - accuracy: 0.8413\nEpoch 488/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4005 - accuracy: 0.8428\nEpoch 489/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4006 - accuracy: 0.8428\nEpoch 490/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4008 - accuracy: 0.8428\nEpoch 491/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4001 - accuracy: 0.8398\nEpoch 492/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4001 - accuracy: 0.8413\nEpoch 493/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4006 - accuracy: 0.8413\nEpoch 494/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4006 - accuracy: 0.8398\nEpoch 495/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.4001 - accuracy: 0.8428\nEpoch 496/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3999 - accuracy: 0.8428\nEpoch 497/1000\n668/668 [==============================] - 0s 73us/step - loss: 0.4002 - accuracy: 0.8413\nEpoch 498/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.4000 - accuracy: 0.8413\nEpoch 499/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3995 - accuracy: 0.8413\nEpoch 500/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3995 - accuracy: 0.8413\nEpoch 501/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3997 - accuracy: 0.8413\nEpoch 502/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3995 - accuracy: 0.8413\nEpoch 503/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4002 - accuracy: 0.8368\nEpoch 504/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.4004 - accuracy: 0.8398\nEpoch 505/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3995 - accuracy: 0.8413\nEpoch 506/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3994 - accuracy: 0.8413\nEpoch 507/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3991 - accuracy: 0.8413\nEpoch 508/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3996 - accuracy: 0.8413\nEpoch 509/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3993 - accuracy: 0.8398\nEpoch 510/1000\n668/668 [==============================] - 0s 76us/step - loss: 0.3996 - accuracy: 0.8428\nEpoch 511/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.4001 - accuracy: 0.8413\nEpoch 512/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3995 - accuracy: 0.8413\nEpoch 513/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.3995 - accuracy: 0.8383\nEpoch 514/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3990 - accuracy: 0.8413\nEpoch 515/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3994 - accuracy: 0.8428\nEpoch 516/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3993 - accuracy: 0.8428\nEpoch 517/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3993 - accuracy: 0.8413\nEpoch 518/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3989 - accuracy: 0.8413\nEpoch 519/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3996 - accuracy: 0.8428\nEpoch 520/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3990 - accuracy: 0.8413\nEpoch 521/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3995 - accuracy: 0.8428\nEpoch 522/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3992 - accuracy: 0.8413\nEpoch 523/1000\n668/668 [==============================] - 0s 74us/step - loss: 0.3991 - accuracy: 0.8398\nEpoch 524/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3990 - accuracy: 0.8413\nEpoch 525/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.3989 - accuracy: 0.8428\nEpoch 526/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3988 - accuracy: 0.8413\nEpoch 527/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3985 - accuracy: 0.8428\nEpoch 528/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3989 - accuracy: 0.8413\nEpoch 529/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.3992 - accuracy: 0.8383\nEpoch 530/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3992 - accuracy: 0.8413\nEpoch 531/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3982 - accuracy: 0.8428\nEpoch 532/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3984 - accuracy: 0.8398\nEpoch 533/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3986 - accuracy: 0.8398\nEpoch 534/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3984 - accuracy: 0.8398\nEpoch 535/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3986 - accuracy: 0.8428\nEpoch 536/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3979 - accuracy: 0.8413\nEpoch 537/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3988 - accuracy: 0.8428\nEpoch 538/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3984 - accuracy: 0.8413\nEpoch 539/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3982 - accuracy: 0.8428\nEpoch 540/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3984 - accuracy: 0.8428\nEpoch 541/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3989 - accuracy: 0.8428\nEpoch 542/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3983 - accuracy: 0.8428\nEpoch 543/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3981 - accuracy: 0.8428\nEpoch 544/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3982 - accuracy: 0.8428\nEpoch 545/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3978 - accuracy: 0.8428\nEpoch 546/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3980 - accuracy: 0.8428\nEpoch 547/1000\n","name":"stdout"},{"output_type":"stream","text":"668/668 [==============================] - 0s 63us/step - loss: 0.3983 - accuracy: 0.8428\nEpoch 548/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3982 - accuracy: 0.8428\nEpoch 549/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3977 - accuracy: 0.8428\nEpoch 550/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3980 - accuracy: 0.8413\nEpoch 551/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3975 - accuracy: 0.8428\nEpoch 552/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3980 - accuracy: 0.8428\nEpoch 553/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3976 - accuracy: 0.8383\nEpoch 554/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3976 - accuracy: 0.8398\nEpoch 555/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3977 - accuracy: 0.8383\nEpoch 556/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3976 - accuracy: 0.8428\nEpoch 557/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3973 - accuracy: 0.8398\nEpoch 558/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3976 - accuracy: 0.8398\nEpoch 559/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3977 - accuracy: 0.8413\nEpoch 560/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3971 - accuracy: 0.8398\nEpoch 561/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3972 - accuracy: 0.8383\nEpoch 562/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3969 - accuracy: 0.8398\nEpoch 563/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3972 - accuracy: 0.8398\nEpoch 564/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3968 - accuracy: 0.8413\nEpoch 565/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3966 - accuracy: 0.8398\nEpoch 566/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3965 - accuracy: 0.8398\nEpoch 567/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3966 - accuracy: 0.8398\nEpoch 568/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3972 - accuracy: 0.8368\nEpoch 569/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3961 - accuracy: 0.8383\nEpoch 570/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3960 - accuracy: 0.8398\nEpoch 571/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3961 - accuracy: 0.8413\nEpoch 572/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3962 - accuracy: 0.8383\nEpoch 573/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3963 - accuracy: 0.8368\nEpoch 574/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3964 - accuracy: 0.8368\nEpoch 575/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3960 - accuracy: 0.8413\nEpoch 576/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3963 - accuracy: 0.8398\nEpoch 577/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3959 - accuracy: 0.8413\nEpoch 578/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3958 - accuracy: 0.8413\nEpoch 579/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3958 - accuracy: 0.8413\nEpoch 580/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3958 - accuracy: 0.8383\nEpoch 581/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3955 - accuracy: 0.8383\nEpoch 582/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3958 - accuracy: 0.8413\nEpoch 583/1000\n668/668 [==============================] - 0s 130us/step - loss: 0.3956 - accuracy: 0.8368\nEpoch 584/1000\n668/668 [==============================] - 0s 90us/step - loss: 0.3954 - accuracy: 0.8428\nEpoch 585/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3954 - accuracy: 0.8428\nEpoch 586/1000\n668/668 [==============================] - 0s 75us/step - loss: 0.3953 - accuracy: 0.8398\nEpoch 587/1000\n668/668 [==============================] - 0s 76us/step - loss: 0.3950 - accuracy: 0.8398\nEpoch 588/1000\n668/668 [==============================] - 0s 75us/step - loss: 0.3952 - accuracy: 0.8368\nEpoch 589/1000\n668/668 [==============================] - 0s 75us/step - loss: 0.3947 - accuracy: 0.8383\nEpoch 590/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3949 - accuracy: 0.8443\nEpoch 591/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3950 - accuracy: 0.8368\nEpoch 592/1000\n668/668 [==============================] - 0s 76us/step - loss: 0.3949 - accuracy: 0.8413\nEpoch 593/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3948 - accuracy: 0.8428\nEpoch 594/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3945 - accuracy: 0.8413\nEpoch 595/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3947 - accuracy: 0.8383\nEpoch 596/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3943 - accuracy: 0.8368\nEpoch 597/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3948 - accuracy: 0.8368\nEpoch 598/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3950 - accuracy: 0.8428\nEpoch 599/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3950 - accuracy: 0.8383\nEpoch 600/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3950 - accuracy: 0.8413\nEpoch 601/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3944 - accuracy: 0.8368\nEpoch 602/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3948 - accuracy: 0.8383\nEpoch 603/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3946 - accuracy: 0.8398\nEpoch 604/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3943 - accuracy: 0.8398\nEpoch 605/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3942 - accuracy: 0.8428\nEpoch 606/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3942 - accuracy: 0.8398\nEpoch 607/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3939 - accuracy: 0.8458\nEpoch 608/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3938 - accuracy: 0.8443\nEpoch 609/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3939 - accuracy: 0.8398\nEpoch 610/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3938 - accuracy: 0.8383\nEpoch 611/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3939 - accuracy: 0.8398\nEpoch 612/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3937 - accuracy: 0.8353\nEpoch 613/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3937 - accuracy: 0.8368\nEpoch 614/1000\n668/668 [==============================] - 0s 88us/step - loss: 0.3942 - accuracy: 0.8338\nEpoch 615/1000\n668/668 [==============================] - 0s 76us/step - loss: 0.3937 - accuracy: 0.8383\nEpoch 616/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3937 - accuracy: 0.8353\nEpoch 617/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3935 - accuracy: 0.8368\nEpoch 618/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3943 - accuracy: 0.8383\nEpoch 619/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3938 - accuracy: 0.8368\nEpoch 620/1000\n668/668 [==============================] - 0s 82us/step - loss: 0.3936 - accuracy: 0.8368\nEpoch 621/1000\n668/668 [==============================] - 0s 100us/step - loss: 0.3939 - accuracy: 0.8383\nEpoch 622/1000\n668/668 [==============================] - 0s 82us/step - loss: 0.3935 - accuracy: 0.8368\nEpoch 623/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3934 - accuracy: 0.8338\nEpoch 624/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3936 - accuracy: 0.8368\nEpoch 625/1000\n","name":"stdout"},{"output_type":"stream","text":"668/668 [==============================] - 0s 64us/step - loss: 0.3937 - accuracy: 0.8383\nEpoch 626/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3943 - accuracy: 0.8383\nEpoch 627/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3929 - accuracy: 0.8338\nEpoch 628/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3935 - accuracy: 0.8338\nEpoch 629/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3933 - accuracy: 0.8383\nEpoch 630/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3936 - accuracy: 0.8353\nEpoch 631/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3936 - accuracy: 0.8383\nEpoch 632/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3930 - accuracy: 0.8368\nEpoch 633/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3933 - accuracy: 0.8338\nEpoch 634/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3930 - accuracy: 0.8383\nEpoch 635/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3941 - accuracy: 0.8383\nEpoch 636/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3931 - accuracy: 0.8383\nEpoch 637/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3927 - accuracy: 0.8383\nEpoch 638/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3932 - accuracy: 0.8413\nEpoch 639/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3932 - accuracy: 0.8383\nEpoch 640/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3933 - accuracy: 0.8368\nEpoch 641/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3928 - accuracy: 0.8383\nEpoch 642/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3935 - accuracy: 0.8383\nEpoch 643/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.3932 - accuracy: 0.8413\nEpoch 644/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3931 - accuracy: 0.8383\nEpoch 645/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3929 - accuracy: 0.8383\nEpoch 646/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3927 - accuracy: 0.8383\nEpoch 647/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3927 - accuracy: 0.8383\nEpoch 648/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3930 - accuracy: 0.8383\nEpoch 649/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3942 - accuracy: 0.8413\nEpoch 650/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3929 - accuracy: 0.8398\nEpoch 651/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3927 - accuracy: 0.8383\nEpoch 652/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3924 - accuracy: 0.8383\nEpoch 653/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3927 - accuracy: 0.8383\nEpoch 654/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3924 - accuracy: 0.8383\nEpoch 655/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3927 - accuracy: 0.8368\nEpoch 656/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3933 - accuracy: 0.8413\nEpoch 657/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3926 - accuracy: 0.8383\nEpoch 658/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3927 - accuracy: 0.8368\nEpoch 659/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3925 - accuracy: 0.8368\nEpoch 660/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3929 - accuracy: 0.8368\nEpoch 661/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3921 - accuracy: 0.8353\nEpoch 662/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3924 - accuracy: 0.8353\nEpoch 663/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3922 - accuracy: 0.8383\nEpoch 664/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3923 - accuracy: 0.8383\nEpoch 665/1000\n668/668 [==============================] - 0s 84us/step - loss: 0.3923 - accuracy: 0.8383\nEpoch 666/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3924 - accuracy: 0.8398\nEpoch 667/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3925 - accuracy: 0.8368\nEpoch 668/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3919 - accuracy: 0.8398\nEpoch 669/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3922 - accuracy: 0.8368\nEpoch 670/1000\n668/668 [==============================] - 0s 83us/step - loss: 0.3919 - accuracy: 0.8398\nEpoch 671/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3921 - accuracy: 0.8368\nEpoch 672/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3924 - accuracy: 0.8383\nEpoch 673/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3920 - accuracy: 0.8383\nEpoch 674/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3918 - accuracy: 0.8398\nEpoch 675/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3922 - accuracy: 0.8383\nEpoch 676/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3922 - accuracy: 0.8398\nEpoch 677/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3917 - accuracy: 0.8368\nEpoch 678/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3919 - accuracy: 0.8368\nEpoch 679/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3920 - accuracy: 0.8413\nEpoch 680/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.3918 - accuracy: 0.8398\nEpoch 681/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3916 - accuracy: 0.8398\nEpoch 682/1000\n668/668 [==============================] - 0s 80us/step - loss: 0.3917 - accuracy: 0.8383\nEpoch 683/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3919 - accuracy: 0.8413\nEpoch 684/1000\n668/668 [==============================] - 0s 74us/step - loss: 0.3914 - accuracy: 0.8383\nEpoch 685/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3919 - accuracy: 0.8413\nEpoch 686/1000\n668/668 [==============================] - 0s 75us/step - loss: 0.3910 - accuracy: 0.8398\nEpoch 687/1000\n668/668 [==============================] - 0s 77us/step - loss: 0.3915 - accuracy: 0.8413\nEpoch 688/1000\n668/668 [==============================] - 0s 73us/step - loss: 0.3916 - accuracy: 0.8398\nEpoch 689/1000\n668/668 [==============================] - 0s 73us/step - loss: 0.3913 - accuracy: 0.8413\nEpoch 690/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3918 - accuracy: 0.8413\nEpoch 691/1000\n668/668 [==============================] - 0s 73us/step - loss: 0.3910 - accuracy: 0.8383\nEpoch 692/1000\n668/668 [==============================] - 0s 79us/step - loss: 0.3911 - accuracy: 0.8383\nEpoch 693/1000\n668/668 [==============================] - 0s 78us/step - loss: 0.3916 - accuracy: 0.8398\nEpoch 694/1000\n668/668 [==============================] - 0s 85us/step - loss: 0.3913 - accuracy: 0.8413\nEpoch 695/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3912 - accuracy: 0.8398\nEpoch 696/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3909 - accuracy: 0.8428\nEpoch 697/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3914 - accuracy: 0.8398\nEpoch 698/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3912 - accuracy: 0.8428\nEpoch 699/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3910 - accuracy: 0.8443\nEpoch 700/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3908 - accuracy: 0.8398\nEpoch 701/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3905 - accuracy: 0.8383\nEpoch 702/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3909 - accuracy: 0.8398\nEpoch 703/1000\n","name":"stdout"},{"output_type":"stream","text":"668/668 [==============================] - 0s 66us/step - loss: 0.3910 - accuracy: 0.8428\nEpoch 704/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3910 - accuracy: 0.8428\nEpoch 705/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3907 - accuracy: 0.8413\nEpoch 706/1000\n668/668 [==============================] - 0s 77us/step - loss: 0.3905 - accuracy: 0.8398\nEpoch 707/1000\n668/668 [==============================] - 0s 75us/step - loss: 0.3907 - accuracy: 0.8398\nEpoch 708/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.3916 - accuracy: 0.8428\nEpoch 709/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3903 - accuracy: 0.8428\nEpoch 710/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3902 - accuracy: 0.8413\nEpoch 711/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3904 - accuracy: 0.8413\nEpoch 712/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3902 - accuracy: 0.8398\nEpoch 713/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3901 - accuracy: 0.8428\nEpoch 714/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3901 - accuracy: 0.8443\nEpoch 715/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3907 - accuracy: 0.8413\nEpoch 716/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3901 - accuracy: 0.8398\nEpoch 717/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3902 - accuracy: 0.8428\nEpoch 718/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3904 - accuracy: 0.8428\nEpoch 719/1000\n668/668 [==============================] - 0s 61us/step - loss: 0.3896 - accuracy: 0.8398\nEpoch 720/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3905 - accuracy: 0.8413\nEpoch 721/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3898 - accuracy: 0.8413\nEpoch 722/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3900 - accuracy: 0.8398\nEpoch 723/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3899 - accuracy: 0.8413\nEpoch 724/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3897 - accuracy: 0.8428\nEpoch 725/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3895 - accuracy: 0.8413\nEpoch 726/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3901 - accuracy: 0.8413\nEpoch 727/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3900 - accuracy: 0.8428\nEpoch 728/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3894 - accuracy: 0.8428\nEpoch 729/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3902 - accuracy: 0.8428\nEpoch 730/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3902 - accuracy: 0.8443\nEpoch 731/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3899 - accuracy: 0.8413\nEpoch 732/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3892 - accuracy: 0.8413\nEpoch 733/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3895 - accuracy: 0.8413\nEpoch 734/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3895 - accuracy: 0.8413\nEpoch 735/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3892 - accuracy: 0.8413\nEpoch 736/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3888 - accuracy: 0.8428\nEpoch 737/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3892 - accuracy: 0.8428\nEpoch 738/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3897 - accuracy: 0.8413\nEpoch 739/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3894 - accuracy: 0.8428\nEpoch 740/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3895 - accuracy: 0.8413\nEpoch 741/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3902 - accuracy: 0.8398\nEpoch 742/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3889 - accuracy: 0.8428\nEpoch 743/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3892 - accuracy: 0.8413\nEpoch 744/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3891 - accuracy: 0.8428\nEpoch 745/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3888 - accuracy: 0.8413\nEpoch 746/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3884 - accuracy: 0.8413\nEpoch 747/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3895 - accuracy: 0.8413\nEpoch 748/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3893 - accuracy: 0.8428\nEpoch 749/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3891 - accuracy: 0.8428\nEpoch 750/1000\n668/668 [==============================] - 0s 82us/step - loss: 0.3882 - accuracy: 0.8413\nEpoch 751/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3883 - accuracy: 0.8428\nEpoch 752/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3888 - accuracy: 0.8443\nEpoch 753/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3888 - accuracy: 0.8413\nEpoch 754/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3882 - accuracy: 0.8428\nEpoch 755/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3879 - accuracy: 0.8428\nEpoch 756/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3885 - accuracy: 0.8428\nEpoch 757/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3882 - accuracy: 0.8428\nEpoch 758/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3880 - accuracy: 0.8428\nEpoch 759/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3887 - accuracy: 0.8428\nEpoch 760/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3883 - accuracy: 0.8428\nEpoch 761/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3880 - accuracy: 0.8428\nEpoch 762/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3882 - accuracy: 0.8428\nEpoch 763/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3887 - accuracy: 0.8428\nEpoch 764/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3876 - accuracy: 0.8413\nEpoch 765/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.3885 - accuracy: 0.8413\nEpoch 766/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3878 - accuracy: 0.8443\nEpoch 767/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3881 - accuracy: 0.8458\nEpoch 768/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3888 - accuracy: 0.8413\nEpoch 769/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3883 - accuracy: 0.8428\nEpoch 770/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3875 - accuracy: 0.8428\nEpoch 771/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3874 - accuracy: 0.8428\nEpoch 772/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3880 - accuracy: 0.8443\nEpoch 773/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3877 - accuracy: 0.8428\nEpoch 774/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3877 - accuracy: 0.8443\nEpoch 775/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3880 - accuracy: 0.8398\nEpoch 776/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3875 - accuracy: 0.8458\nEpoch 777/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3875 - accuracy: 0.8428\nEpoch 778/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3875 - accuracy: 0.8413\nEpoch 779/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3870 - accuracy: 0.8443\nEpoch 780/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3873 - accuracy: 0.8428\nEpoch 781/1000\n","name":"stdout"},{"output_type":"stream","text":"668/668 [==============================] - 0s 66us/step - loss: 0.3873 - accuracy: 0.8413\nEpoch 782/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3871 - accuracy: 0.8428\nEpoch 783/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3870 - accuracy: 0.8413\nEpoch 784/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3873 - accuracy: 0.8428\nEpoch 785/1000\n668/668 [==============================] - 0s 76us/step - loss: 0.3870 - accuracy: 0.8413\nEpoch 786/1000\n668/668 [==============================] - 0s 73us/step - loss: 0.3871 - accuracy: 0.8413\nEpoch 787/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3868 - accuracy: 0.8398\nEpoch 788/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3872 - accuracy: 0.8398\nEpoch 789/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3875 - accuracy: 0.8428\nEpoch 790/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3870 - accuracy: 0.8428\nEpoch 791/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3875 - accuracy: 0.8428\nEpoch 792/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3875 - accuracy: 0.8413\nEpoch 793/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3875 - accuracy: 0.8428\nEpoch 794/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3872 - accuracy: 0.8458\nEpoch 795/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3877 - accuracy: 0.8398\nEpoch 796/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3868 - accuracy: 0.8413\nEpoch 797/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3869 - accuracy: 0.8458\nEpoch 798/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3871 - accuracy: 0.8413\nEpoch 799/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3869 - accuracy: 0.8398\nEpoch 800/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3869 - accuracy: 0.8413\nEpoch 801/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3872 - accuracy: 0.8428\nEpoch 802/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3868 - accuracy: 0.8428\nEpoch 803/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3871 - accuracy: 0.8428\nEpoch 804/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3876 - accuracy: 0.8413\nEpoch 805/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3874 - accuracy: 0.8428\nEpoch 806/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3871 - accuracy: 0.8458\nEpoch 807/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3866 - accuracy: 0.8428\nEpoch 808/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3868 - accuracy: 0.8458\nEpoch 809/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3874 - accuracy: 0.8458\nEpoch 810/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3864 - accuracy: 0.8443\nEpoch 811/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3864 - accuracy: 0.8413\nEpoch 812/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3867 - accuracy: 0.8413\nEpoch 813/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3869 - accuracy: 0.8428\nEpoch 814/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3866 - accuracy: 0.8458\nEpoch 815/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3865 - accuracy: 0.8458\nEpoch 816/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3864 - accuracy: 0.8443\nEpoch 817/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3864 - accuracy: 0.8458\nEpoch 818/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3866 - accuracy: 0.8443\nEpoch 819/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3862 - accuracy: 0.8458\nEpoch 820/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3869 - accuracy: 0.8413\nEpoch 821/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3864 - accuracy: 0.8443\nEpoch 822/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3861 - accuracy: 0.8413\nEpoch 823/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3867 - accuracy: 0.8428\nEpoch 824/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3868 - accuracy: 0.8443\nEpoch 825/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3868 - accuracy: 0.8428\nEpoch 826/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3863 - accuracy: 0.8428\nEpoch 827/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3863 - accuracy: 0.8443\nEpoch 828/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3859 - accuracy: 0.8398\nEpoch 829/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3860 - accuracy: 0.8383\nEpoch 830/1000\n668/668 [==============================] - 0s 61us/step - loss: 0.3861 - accuracy: 0.8428\nEpoch 831/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3864 - accuracy: 0.8428\nEpoch 832/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3864 - accuracy: 0.8413\nEpoch 833/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.3864 - accuracy: 0.8428\nEpoch 834/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3863 - accuracy: 0.8428\nEpoch 835/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3862 - accuracy: 0.8443\nEpoch 836/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3861 - accuracy: 0.8428\nEpoch 837/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3863 - accuracy: 0.8458\nEpoch 838/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3861 - accuracy: 0.8398\nEpoch 839/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3862 - accuracy: 0.8443\nEpoch 840/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3869 - accuracy: 0.8428\nEpoch 841/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3864 - accuracy: 0.8428\nEpoch 842/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3856 - accuracy: 0.8458\nEpoch 843/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3859 - accuracy: 0.8443\nEpoch 844/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3862 - accuracy: 0.8413\nEpoch 845/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3860 - accuracy: 0.8428\nEpoch 846/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3865 - accuracy: 0.8413\nEpoch 847/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3859 - accuracy: 0.8413\nEpoch 848/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3860 - accuracy: 0.8473\nEpoch 849/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3861 - accuracy: 0.8473\nEpoch 850/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3864 - accuracy: 0.8428\nEpoch 851/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3856 - accuracy: 0.8428\nEpoch 852/1000\n668/668 [==============================] - 0s 73us/step - loss: 0.3860 - accuracy: 0.8443\nEpoch 853/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3855 - accuracy: 0.8443\nEpoch 854/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3860 - accuracy: 0.8458\nEpoch 855/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3857 - accuracy: 0.8428\nEpoch 856/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3860 - accuracy: 0.8383\nEpoch 857/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3854 - accuracy: 0.8398\nEpoch 858/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3856 - accuracy: 0.8443\nEpoch 859/1000\n","name":"stdout"},{"output_type":"stream","text":"668/668 [==============================] - 0s 64us/step - loss: 0.3859 - accuracy: 0.8398\nEpoch 860/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3863 - accuracy: 0.8473\nEpoch 861/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3858 - accuracy: 0.8488\nEpoch 862/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3858 - accuracy: 0.8458\nEpoch 863/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3856 - accuracy: 0.8428\nEpoch 864/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3854 - accuracy: 0.8473\nEpoch 865/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3853 - accuracy: 0.8413\nEpoch 866/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3851 - accuracy: 0.8428\nEpoch 867/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3856 - accuracy: 0.8458\nEpoch 868/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3860 - accuracy: 0.8428\nEpoch 869/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3854 - accuracy: 0.8443\nEpoch 870/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3856 - accuracy: 0.8458\nEpoch 871/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3854 - accuracy: 0.8413\nEpoch 872/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3853 - accuracy: 0.8473\nEpoch 873/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3856 - accuracy: 0.8443\nEpoch 874/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3866 - accuracy: 0.8443\nEpoch 875/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3855 - accuracy: 0.8473\nEpoch 876/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3852 - accuracy: 0.8443\nEpoch 877/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3853 - accuracy: 0.8473\nEpoch 878/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3852 - accuracy: 0.8458\nEpoch 879/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3857 - accuracy: 0.8488\nEpoch 880/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3850 - accuracy: 0.8458\nEpoch 881/1000\n668/668 [==============================] - 0s 72us/step - loss: 0.3855 - accuracy: 0.8413\nEpoch 882/1000\n668/668 [==============================] - 0s 70us/step - loss: 0.3850 - accuracy: 0.8428\nEpoch 883/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3853 - accuracy: 0.8428\nEpoch 884/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3852 - accuracy: 0.8443\nEpoch 885/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3848 - accuracy: 0.8488\nEpoch 886/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3850 - accuracy: 0.8503\nEpoch 887/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3855 - accuracy: 0.8443\nEpoch 888/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3850 - accuracy: 0.8443\nEpoch 889/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3852 - accuracy: 0.8443\nEpoch 890/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3849 - accuracy: 0.8473\nEpoch 891/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3850 - accuracy: 0.8488\nEpoch 892/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3854 - accuracy: 0.8473\nEpoch 893/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3851 - accuracy: 0.8458\nEpoch 894/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3859 - accuracy: 0.8458\nEpoch 895/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3850 - accuracy: 0.8458\nEpoch 896/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3848 - accuracy: 0.8458\nEpoch 897/1000\n668/668 [==============================] - 0s 71us/step - loss: 0.3850 - accuracy: 0.8473\nEpoch 898/1000\n668/668 [==============================] - 0s 82us/step - loss: 0.3849 - accuracy: 0.8473\nEpoch 899/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3849 - accuracy: 0.8488\nEpoch 900/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3863 - accuracy: 0.8443\nEpoch 901/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3854 - accuracy: 0.8473\nEpoch 902/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3845 - accuracy: 0.8473\nEpoch 903/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3847 - accuracy: 0.8443\nEpoch 904/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3850 - accuracy: 0.8443\nEpoch 905/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3855 - accuracy: 0.8443\nEpoch 906/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3844 - accuracy: 0.8443\nEpoch 907/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3844 - accuracy: 0.8473\nEpoch 908/1000\n668/668 [==============================] - 0s 75us/step - loss: 0.3847 - accuracy: 0.8443\nEpoch 909/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3849 - accuracy: 0.8473\nEpoch 910/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3847 - accuracy: 0.8443\nEpoch 911/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3850 - accuracy: 0.8458\nEpoch 912/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3845 - accuracy: 0.8428\nEpoch 913/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3846 - accuracy: 0.8473\nEpoch 914/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3847 - accuracy: 0.8473\nEpoch 915/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3845 - accuracy: 0.8428\nEpoch 916/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3858 - accuracy: 0.8458\nEpoch 917/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3852 - accuracy: 0.8443\nEpoch 918/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3846 - accuracy: 0.8428\nEpoch 919/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3848 - accuracy: 0.8443\nEpoch 920/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3848 - accuracy: 0.8473\nEpoch 921/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3847 - accuracy: 0.8473\nEpoch 922/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3847 - accuracy: 0.8473\nEpoch 923/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3846 - accuracy: 0.8443\nEpoch 924/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3850 - accuracy: 0.8458\nEpoch 925/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3846 - accuracy: 0.8443\nEpoch 926/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3846 - accuracy: 0.8458\nEpoch 927/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3844 - accuracy: 0.8473\nEpoch 928/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3844 - accuracy: 0.8458\nEpoch 929/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3846 - accuracy: 0.8458\nEpoch 930/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3842 - accuracy: 0.8488\nEpoch 931/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3851 - accuracy: 0.8458\nEpoch 932/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3844 - accuracy: 0.8473\nEpoch 933/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3850 - accuracy: 0.8428\nEpoch 934/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3844 - accuracy: 0.8443\nEpoch 935/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3846 - accuracy: 0.8458\nEpoch 936/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3854 - accuracy: 0.8458\nEpoch 937/1000\n","name":"stdout"},{"output_type":"stream","text":"668/668 [==============================] - 0s 64us/step - loss: 0.3845 - accuracy: 0.8458\nEpoch 938/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3849 - accuracy: 0.8458\nEpoch 939/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3847 - accuracy: 0.8413\nEpoch 940/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3846 - accuracy: 0.8428\nEpoch 941/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3843 - accuracy: 0.8443\nEpoch 942/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3843 - accuracy: 0.8473\nEpoch 943/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3842 - accuracy: 0.8473\nEpoch 944/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3845 - accuracy: 0.8473\nEpoch 945/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3853 - accuracy: 0.8488\nEpoch 946/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3843 - accuracy: 0.8458\nEpoch 947/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3841 - accuracy: 0.8458\nEpoch 948/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3844 - accuracy: 0.8443\nEpoch 949/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3846 - accuracy: 0.8443\nEpoch 950/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3843 - accuracy: 0.8458\nEpoch 951/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3838 - accuracy: 0.8458\nEpoch 952/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3845 - accuracy: 0.8413\nEpoch 953/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3841 - accuracy: 0.8443\nEpoch 954/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3842 - accuracy: 0.8473\nEpoch 955/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3845 - accuracy: 0.8473\nEpoch 956/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3840 - accuracy: 0.8443\nEpoch 957/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3850 - accuracy: 0.8473\nEpoch 958/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3841 - accuracy: 0.8458\nEpoch 959/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3840 - accuracy: 0.8473\nEpoch 960/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3841 - accuracy: 0.8488\nEpoch 961/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3844 - accuracy: 0.8413\nEpoch 962/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3846 - accuracy: 0.8413\nEpoch 963/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3842 - accuracy: 0.8443\nEpoch 964/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3845 - accuracy: 0.8473\nEpoch 965/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3845 - accuracy: 0.8473\nEpoch 966/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3841 - accuracy: 0.8458\nEpoch 967/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3846 - accuracy: 0.8458\nEpoch 968/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3845 - accuracy: 0.8413\nEpoch 969/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3843 - accuracy: 0.8488\nEpoch 970/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3849 - accuracy: 0.8428\nEpoch 971/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3849 - accuracy: 0.8428\nEpoch 972/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3841 - accuracy: 0.8458\nEpoch 973/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3839 - accuracy: 0.8443\nEpoch 974/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3840 - accuracy: 0.8473\nEpoch 975/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3839 - accuracy: 0.8488\nEpoch 976/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3841 - accuracy: 0.8473\nEpoch 977/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3835 - accuracy: 0.8503\nEpoch 978/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3847 - accuracy: 0.8428\nEpoch 979/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3840 - accuracy: 0.8458\nEpoch 980/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3839 - accuracy: 0.8473\nEpoch 981/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3840 - accuracy: 0.8458\nEpoch 982/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3836 - accuracy: 0.8443\nEpoch 983/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3845 - accuracy: 0.8488\nEpoch 984/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3838 - accuracy: 0.8443\nEpoch 985/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3841 - accuracy: 0.8428\nEpoch 986/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3843 - accuracy: 0.8443\nEpoch 987/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3838 - accuracy: 0.8488\nEpoch 988/1000\n668/668 [==============================] - 0s 63us/step - loss: 0.3845 - accuracy: 0.8458\nEpoch 989/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3833 - accuracy: 0.8488\nEpoch 990/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3846 - accuracy: 0.8458\nEpoch 991/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3843 - accuracy: 0.8473\nEpoch 992/1000\n668/668 [==============================] - 0s 62us/step - loss: 0.3833 - accuracy: 0.8458\nEpoch 993/1000\n668/668 [==============================] - 0s 64us/step - loss: 0.3845 - accuracy: 0.8458\nEpoch 994/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3845 - accuracy: 0.8443\nEpoch 995/1000\n668/668 [==============================] - 0s 69us/step - loss: 0.3842 - accuracy: 0.8473\nEpoch 996/1000\n668/668 [==============================] - 0s 66us/step - loss: 0.3837 - accuracy: 0.8458\nEpoch 997/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3842 - accuracy: 0.8458\nEpoch 998/1000\n668/668 [==============================] - 0s 67us/step - loss: 0.3845 - accuracy: 0.8458\nEpoch 999/1000\n668/668 [==============================] - 0s 68us/step - loss: 0.3840 - accuracy: 0.8473\nEpoch 1000/1000\n668/668 [==============================] - 0s 65us/step - loss: 0.3842 - accuracy: 0.8443\n","name":"stdout"},{"output_type":"execute_result","execution_count":119,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7fec2c4d89b0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=classifier.predict(x_test)","execution_count":120,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":121,"outputs":[{"output_type":"execute_result","execution_count":121,"data":{"text/plain":"array([[0.17538437],\n       [0.09540212],\n       [0.13887672],\n       [0.9629195 ],\n       [0.49011293],\n       [0.30441213],\n       [0.9485623 ],\n       [0.8648216 ],\n       [0.403567  ],\n       [0.7823166 ],\n       [0.06794314],\n       [0.8875945 ],\n       [0.14467971],\n       [0.94273293],\n       [0.9616677 ],\n       [0.64554465],\n       [0.12133033],\n       [0.51306057],\n       [0.08090086],\n       [0.81387776],\n       [0.19561711],\n       [0.8997102 ],\n       [0.1446607 ],\n       [0.30181476],\n       [0.64085424],\n       [0.95990247],\n       [0.0886178 ],\n       [0.63603663],\n       [0.9223189 ],\n       [0.41777176],\n       [0.16662268],\n       [0.7880878 ],\n       [0.10318843],\n       [0.27902007],\n       [0.06064674],\n       [0.32009277],\n       [0.07105181],\n       [0.12540728],\n       [0.13513564],\n       [0.08657517],\n       [0.2727486 ],\n       [0.18835253],\n       [0.10324516],\n       [0.0314474 ],\n       [0.9898358 ],\n       [0.09576958],\n       [0.09576958],\n       [0.6030934 ],\n       [0.10871243],\n       [0.27924022],\n       [0.30702177],\n       [0.3002211 ],\n       [0.97807515],\n       [0.13615395],\n       [0.33478862],\n       [0.11754936],\n       [0.11959923],\n       [0.09094895],\n       [0.14253366],\n       [0.06865977],\n       [0.19704829],\n       [0.5279215 ],\n       [0.9883206 ],\n       [0.2829142 ],\n       [0.46232063],\n       [0.13441844],\n       [0.98198134],\n       [0.1636876 ],\n       [0.97793245],\n       [0.98914003],\n       [0.95888853],\n       [0.25010216],\n       [0.44202894],\n       [0.09593385],\n       [0.11404549],\n       [0.90373516],\n       [0.285729  ],\n       [0.29971522],\n       [0.08149925],\n       [0.14778388],\n       [0.1104764 ],\n       [0.4185099 ],\n       [0.95983076],\n       [0.1496146 ],\n       [0.11333717],\n       [0.9641368 ],\n       [0.98268306],\n       [0.3166888 ],\n       [0.9225986 ],\n       [0.33500093],\n       [0.480918  ],\n       [0.13615395],\n       [0.95381355],\n       [0.96325   ],\n       [0.2480081 ],\n       [0.17532927],\n       [0.90795135],\n       [0.0301503 ],\n       [0.12337512],\n       [0.1560518 ],\n       [0.02088838],\n       [0.06389774],\n       [0.13579896],\n       [0.21595478],\n       [0.24473414],\n       [0.24766621],\n       [0.9482127 ],\n       [0.285729  ],\n       [0.10234687],\n       [0.67201376],\n       [0.06777146],\n       [0.9751414 ],\n       [0.1640151 ],\n       [0.5001825 ],\n       [0.1798134 ],\n       [0.9477884 ],\n       [0.46013576],\n       [0.9643565 ],\n       [0.07551203],\n       [0.85729635],\n       [0.16906075],\n       [0.14576042],\n       [0.13292505],\n       [0.3366291 ],\n       [0.08639544],\n       [0.22002195],\n       [0.17532924],\n       [0.07027058],\n       [0.10811705],\n       [0.08927789],\n       [0.74107337],\n       [0.12785888],\n       [0.11096344],\n       [0.45730475],\n       [0.11518555],\n       [0.12785888],\n       [0.09216617],\n       [0.2795817 ],\n       [0.07533157],\n       [0.13932437],\n       [0.12905216],\n       [0.9892195 ],\n       [0.09593385],\n       [0.71391034],\n       [0.9146874 ],\n       [0.60434604],\n       [0.12512736],\n       [0.86284715],\n       [0.9848127 ],\n       [0.09658879],\n       [0.3245682 ],\n       [0.50576663],\n       [0.40066364],\n       [0.09791157],\n       [0.96333337],\n       [0.1293396 ],\n       [0.70937914],\n       [0.07285296],\n       [0.7654496 ],\n       [0.9322246 ],\n       [0.09576958],\n       [0.23699182],\n       [0.99262404],\n       [0.1530637 ],\n       [0.8166913 ],\n       [0.11169308],\n       [0.06433263],\n       [0.12343414],\n       [0.11574195],\n       [0.14630586],\n       [0.06526139],\n       [0.99287647],\n       [0.09508435],\n       [0.10324516],\n       [0.65886104],\n       [0.09593385],\n       [0.9828388 ],\n       [0.11954544],\n       [0.13637702],\n       [0.4798618 ],\n       [0.11970117],\n       [0.48294914],\n       [0.25464478],\n       [0.12676635],\n       [0.24966083],\n       [0.12377261],\n       [0.7656814 ],\n       [0.10811705],\n       [0.10820764],\n       [0.9639608 ],\n       [0.45866236],\n       [0.5348139 ],\n       [0.63204074],\n       [0.7654271 ],\n       [0.14467971],\n       [0.15521936],\n       [0.09593385],\n       [0.76238585],\n       [0.3063534 ],\n       [0.05970367],\n       [0.14081253],\n       [0.8269909 ],\n       [0.06404155],\n       [0.80321616],\n       [0.13637702],\n       [0.33397734],\n       [0.44864884],\n       [0.08908221],\n       [0.68202895],\n       [0.06615034],\n       [0.11468217],\n       [0.20306455],\n       [0.04264023],\n       [0.86068606],\n       [0.11843467],\n       [0.13615395],\n       [0.29097047],\n       [0.7595841 ],\n       [0.14446281],\n       [0.9186409 ],\n       [0.13597937],\n       [0.68307143],\n       [0.6889107 ]], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":122,"outputs":[{"output_type":"execute_result","execution_count":122,"data":{"text/plain":"array([[0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"(y_pred > 0.5).astype(int)","execution_count":123,"outputs":[{"output_type":"execute_result","execution_count":123,"data":{"text/plain":"array([[0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix","execution_count":124,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dp=pd.read_csv(\"../input/titanic/test.csv\")","execution_count":126,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dp.head()","execution_count":127,"outputs":[{"output_type":"execute_result","execution_count":127,"data":{"text/plain":"   PassengerId  Pclass                                          Name     Sex  \\\n0          892       3                              Kelly, Mr. James    male   \n1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n2          894       2                     Myles, Mr. Thomas Francis    male   \n3          895       3                              Wirz, Mr. Albert    male   \n4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n\n    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n0  34.5      0      0   330911   7.8292   NaN        Q  \n1  47.0      1      0   363272   7.0000   NaN        S  \n2  62.0      0      0   240276   9.6875   NaN        Q  \n3  27.0      0      0   315154   8.6625   NaN        S  \n4  22.0      1      1  3101298  12.2875   NaN        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>3</td>\n      <td>Kelly, Mr. James</td>\n      <td>male</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330911</td>\n      <td>7.8292</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>3</td>\n      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n      <td>female</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>363272</td>\n      <td>7.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>2</td>\n      <td>Myles, Mr. Thomas Francis</td>\n      <td>male</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>240276</td>\n      <td>9.6875</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>3</td>\n      <td>Wirz, Mr. Albert</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>315154</td>\n      <td>8.6625</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>3</td>\n      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3101298</td>\n      <td>12.2875</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1=dp.iloc[:,0:1].values","execution_count":148,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1","execution_count":149,"outputs":[{"output_type":"execute_result","execution_count":149,"data":{"text/plain":"array([[ 892],\n       [ 893],\n       [ 894],\n       [ 895],\n       [ 896],\n       [ 897],\n       [ 898],\n       [ 899],\n       [ 900],\n       [ 901],\n       [ 902],\n       [ 903],\n       [ 904],\n       [ 905],\n       [ 906],\n       [ 907],\n       [ 908],\n       [ 909],\n       [ 910],\n       [ 911],\n       [ 912],\n       [ 913],\n       [ 914],\n       [ 915],\n       [ 916],\n       [ 917],\n       [ 918],\n       [ 919],\n       [ 920],\n       [ 921],\n       [ 922],\n       [ 923],\n       [ 924],\n       [ 925],\n       [ 926],\n       [ 927],\n       [ 928],\n       [ 929],\n       [ 930],\n       [ 931],\n       [ 932],\n       [ 933],\n       [ 934],\n       [ 935],\n       [ 936],\n       [ 937],\n       [ 938],\n       [ 939],\n       [ 940],\n       [ 941],\n       [ 942],\n       [ 943],\n       [ 944],\n       [ 945],\n       [ 946],\n       [ 947],\n       [ 948],\n       [ 949],\n       [ 950],\n       [ 951],\n       [ 952],\n       [ 953],\n       [ 954],\n       [ 955],\n       [ 956],\n       [ 957],\n       [ 958],\n       [ 959],\n       [ 960],\n       [ 961],\n       [ 962],\n       [ 963],\n       [ 964],\n       [ 965],\n       [ 966],\n       [ 967],\n       [ 968],\n       [ 969],\n       [ 970],\n       [ 971],\n       [ 972],\n       [ 973],\n       [ 974],\n       [ 975],\n       [ 976],\n       [ 977],\n       [ 978],\n       [ 979],\n       [ 980],\n       [ 981],\n       [ 982],\n       [ 983],\n       [ 984],\n       [ 985],\n       [ 986],\n       [ 987],\n       [ 988],\n       [ 989],\n       [ 990],\n       [ 991],\n       [ 992],\n       [ 993],\n       [ 994],\n       [ 995],\n       [ 996],\n       [ 997],\n       [ 998],\n       [ 999],\n       [1000],\n       [1001],\n       [1002],\n       [1003],\n       [1004],\n       [1005],\n       [1006],\n       [1007],\n       [1008],\n       [1009],\n       [1010],\n       [1011],\n       [1012],\n       [1013],\n       [1014],\n       [1015],\n       [1016],\n       [1017],\n       [1018],\n       [1019],\n       [1020],\n       [1021],\n       [1022],\n       [1023],\n       [1024],\n       [1025],\n       [1026],\n       [1027],\n       [1028],\n       [1029],\n       [1030],\n       [1031],\n       [1032],\n       [1033],\n       [1034],\n       [1035],\n       [1036],\n       [1037],\n       [1038],\n       [1039],\n       [1040],\n       [1041],\n       [1042],\n       [1043],\n       [1044],\n       [1045],\n       [1046],\n       [1047],\n       [1048],\n       [1049],\n       [1050],\n       [1051],\n       [1052],\n       [1053],\n       [1054],\n       [1055],\n       [1056],\n       [1057],\n       [1058],\n       [1059],\n       [1060],\n       [1061],\n       [1062],\n       [1063],\n       [1064],\n       [1065],\n       [1066],\n       [1067],\n       [1068],\n       [1069],\n       [1070],\n       [1071],\n       [1072],\n       [1073],\n       [1074],\n       [1075],\n       [1076],\n       [1077],\n       [1078],\n       [1079],\n       [1080],\n       [1081],\n       [1082],\n       [1083],\n       [1084],\n       [1085],\n       [1086],\n       [1087],\n       [1088],\n       [1089],\n       [1090],\n       [1091],\n       [1092],\n       [1093],\n       [1094],\n       [1095],\n       [1096],\n       [1097],\n       [1098],\n       [1099],\n       [1100],\n       [1101],\n       [1102],\n       [1103],\n       [1104],\n       [1105],\n       [1106],\n       [1107],\n       [1108],\n       [1109],\n       [1110],\n       [1111],\n       [1112],\n       [1113],\n       [1114],\n       [1115],\n       [1116],\n       [1117],\n       [1118],\n       [1119],\n       [1120],\n       [1121],\n       [1122],\n       [1123],\n       [1124],\n       [1125],\n       [1126],\n       [1127],\n       [1128],\n       [1129],\n       [1130],\n       [1131],\n       [1132],\n       [1133],\n       [1134],\n       [1135],\n       [1136],\n       [1137],\n       [1138],\n       [1139],\n       [1140],\n       [1141],\n       [1142],\n       [1143],\n       [1144],\n       [1145],\n       [1146],\n       [1147],\n       [1148],\n       [1149],\n       [1150],\n       [1151],\n       [1152],\n       [1153],\n       [1154],\n       [1155],\n       [1156],\n       [1157],\n       [1158],\n       [1159],\n       [1160],\n       [1161],\n       [1162],\n       [1163],\n       [1164],\n       [1165],\n       [1166],\n       [1167],\n       [1168],\n       [1169],\n       [1170],\n       [1171],\n       [1172],\n       [1173],\n       [1174],\n       [1175],\n       [1176],\n       [1177],\n       [1178],\n       [1179],\n       [1180],\n       [1181],\n       [1182],\n       [1183],\n       [1184],\n       [1185],\n       [1186],\n       [1187],\n       [1188],\n       [1189],\n       [1190],\n       [1191],\n       [1192],\n       [1193],\n       [1194],\n       [1195],\n       [1196],\n       [1197],\n       [1198],\n       [1199],\n       [1200],\n       [1201],\n       [1202],\n       [1203],\n       [1204],\n       [1205],\n       [1206],\n       [1207],\n       [1208],\n       [1209],\n       [1210],\n       [1211],\n       [1212],\n       [1213],\n       [1214],\n       [1215],\n       [1216],\n       [1217],\n       [1218],\n       [1219],\n       [1220],\n       [1221],\n       [1222],\n       [1223],\n       [1224],\n       [1225],\n       [1226],\n       [1227],\n       [1228],\n       [1229],\n       [1230],\n       [1231],\n       [1232],\n       [1233],\n       [1234],\n       [1235],\n       [1236],\n       [1237],\n       [1238],\n       [1239],\n       [1240],\n       [1241],\n       [1242],\n       [1243],\n       [1244],\n       [1245],\n       [1246],\n       [1247],\n       [1248],\n       [1249],\n       [1250],\n       [1251],\n       [1252],\n       [1253],\n       [1254],\n       [1255],\n       [1256],\n       [1257],\n       [1258],\n       [1259],\n       [1260],\n       [1261],\n       [1262],\n       [1263],\n       [1264],\n       [1265],\n       [1266],\n       [1267],\n       [1268],\n       [1269],\n       [1270],\n       [1271],\n       [1272],\n       [1273],\n       [1274],\n       [1275],\n       [1276],\n       [1277],\n       [1278],\n       [1279],\n       [1280],\n       [1281],\n       [1282],\n       [1283],\n       [1284],\n       [1285],\n       [1286],\n       [1287],\n       [1288],\n       [1289],\n       [1290],\n       [1291],\n       [1292],\n       [1293],\n       [1294],\n       [1295],\n       [1296],\n       [1297],\n       [1298],\n       [1299],\n       [1300],\n       [1301],\n       [1302],\n       [1303],\n       [1304],\n       [1305],\n       [1306],\n       [1307],\n       [1308],\n       [1309]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"t=dp.drop(['PassengerId','Name','Cabin','Ticket'],axis=1)","execution_count":131,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.head()","execution_count":132,"outputs":[{"output_type":"execute_result","execution_count":132,"data":{"text/plain":"   Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n0       3    male  34.5      0      0   7.8292        Q\n1       3  female  47.0      1      0   7.0000        S\n2       2    male  62.0      0      0   9.6875        Q\n3       3    male  27.0      0      0   8.6625        S\n4       3  female  22.0      1      1  12.2875        S","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>male</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.8292</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>female</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.0000</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>male</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9.6875</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.6625</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>12.2875</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"t['Sex']=le.fit_transform(t.Sex)","execution_count":133,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t['Embarked']=le.fit_transform(t.Embarked)","execution_count":134,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.head()","execution_count":135,"outputs":[{"output_type":"execute_result","execution_count":135,"data":{"text/plain":"   Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n0       3    1  34.5      0      0   7.8292         1\n1       3    0  47.0      1      0   7.0000         2\n2       2    1  62.0      0      0   9.6875         1\n3       3    1  27.0      0      0   8.6625         2\n4       3    0  22.0      1      1  12.2875         2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>1</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.8292</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>0</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.0000</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9.6875</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.6625</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>0</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>12.2875</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"t=t.iloc[:,:].values","execution_count":136,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t=sc.fit_transform(t)","execution_count":137,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predd=classifier.predict(t)","execution_count":138,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predd","execution_count":139,"outputs":[{"output_type":"execute_result","execution_count":139,"data":{"text/plain":"array([[1.20032571e-01],\n       [1.47393912e-01],\n       [1.01227209e-01],\n       [1.04080170e-01],\n       [1.67398199e-01],\n       [1.53872877e-01],\n       [6.74042106e-01],\n       [1.37737706e-01],\n       [6.02540135e-01],\n       [4.31244522e-02],\n       [           nan],\n       [2.30606899e-01],\n       [9.94709969e-01],\n       [4.72536981e-02],\n       [9.85808671e-01],\n       [9.91019666e-01],\n       [1.43444955e-01],\n       [1.97130039e-01],\n       [3.25053722e-01],\n       [7.37345815e-01],\n       [5.51357448e-01],\n       [5.19748330e-01],\n       [           nan],\n       [3.11968148e-01],\n       [9.23290431e-01],\n       [3.67809087e-02],\n       [9.69560921e-01],\n       [1.93114132e-01],\n       [2.29421720e-01],\n       [           nan],\n       [5.76803274e-02],\n       [5.03607690e-02],\n       [1.07365571e-01],\n       [           nan],\n       [6.70571566e-01],\n       [2.34976694e-01],\n       [           nan],\n       [5.66034913e-01],\n       [1.11512832e-01],\n       [           nan],\n       [1.82999879e-01],\n       [           nan],\n       [7.27686659e-02],\n       [9.21162307e-01],\n       [9.86559272e-01],\n       [1.09786741e-01],\n       [2.38627523e-01],\n       [           nan],\n       [9.28937554e-01],\n       [2.06773236e-01],\n       [5.73877931e-01],\n       [1.51344255e-01],\n       [8.57293665e-01],\n       [9.41230357e-01],\n       [           nan],\n       [2.20893845e-02],\n       [8.11750293e-02],\n       [1.09487772e-01],\n       [           nan],\n       [9.78309989e-01],\n       [1.39275730e-01],\n       [1.16997361e-01],\n       [1.35060892e-01],\n       [7.03436971e-01],\n       [2.85728991e-01],\n       [           nan],\n       [6.56356096e-01],\n       [1.69525176e-01],\n       [2.94771641e-01],\n       [9.55623150e-01],\n       [7.24853873e-01],\n       [1.23761445e-01],\n       [4.31983143e-01],\n       [3.08457702e-01],\n       [9.57363665e-01],\n       [1.91884384e-01],\n       [           nan],\n       [9.57401335e-01],\n       [1.19754426e-01],\n       [7.24853873e-01],\n       [8.15125346e-01],\n       [6.22376859e-01],\n       [1.97669387e-01],\n       [           nan],\n       [           nan],\n       [           nan],\n       [7.16787040e-01],\n       [5.64909339e-01],\n       [           nan],\n       [9.08487976e-01],\n       [3.17545980e-01],\n       [           nan],\n       [9.96155798e-01],\n       [           nan],\n       [3.31036597e-01],\n       [1.09646223e-01],\n       [9.21985447e-01],\n       [9.71864313e-02],\n       [5.67443132e-01],\n       [8.60894993e-02],\n       [9.96197879e-01],\n       [8.15702900e-02],\n       [           nan],\n       [1.06349610e-01],\n       [7.45851099e-01],\n       [1.17510520e-01],\n       [1.77417278e-01],\n       [           nan],\n       [           nan],\n       [1.61452532e-01],\n       [1.45401582e-01],\n       [           nan],\n       [9.82252359e-01],\n       [6.66668177e-01],\n       [9.93236721e-01],\n       [3.55807424e-01],\n       [           nan],\n       [5.88210464e-01],\n       [1.52526379e-01],\n       [9.14378226e-01],\n       [9.46411908e-01],\n       [           nan],\n       [9.96552587e-01],\n       [9.95341763e-02],\n       [           nan],\n       [4.90846843e-01],\n       [1.20004520e-01],\n       [           nan],\n       [1.00745969e-01],\n       [1.13292918e-01],\n       [8.87118578e-02],\n       [2.17006892e-01],\n       [           nan],\n       [           nan],\n       [7.10431859e-02],\n       [1.13074049e-01],\n       [1.82710245e-01],\n       [1.26748115e-01],\n       [5.31694829e-01],\n       [1.51494756e-01],\n       [9.27954912e-03],\n       [8.98974895e-01],\n       [4.28643852e-01],\n       [1.40912622e-01],\n       [2.41900802e-01],\n       [1.52224367e-02],\n       [           nan],\n       [1.20303847e-01],\n       [           nan],\n       [9.02437791e-02],\n       [9.51224923e-01],\n       [           nan],\n       [           nan],\n       [2.07735151e-01],\n       [1.45767685e-02],\n       [1.12734728e-01],\n       [9.57394242e-01],\n       [5.31253099e-01],\n       [2.41900802e-01],\n       [3.97775114e-01],\n       [           nan],\n       [7.99323857e-01],\n       [9.34593141e-01],\n       [           nan],\n       [1.02221638e-01],\n       [1.68050721e-01],\n       [1.57135904e-01],\n       [6.52487502e-02],\n       [           nan],\n       [5.50282598e-01],\n       [           nan],\n       [1.81440830e-01],\n       [6.78586662e-02],\n       [           nan],\n       [1.21746644e-01],\n       [8.42653275e-01],\n       [9.04973924e-01],\n       [5.77632725e-01],\n       [7.31717587e-01],\n       [8.87341499e-01],\n       [1.19754426e-01],\n       [4.68613237e-01],\n       [9.97477591e-01],\n       [           nan],\n       [9.48292434e-01],\n       [1.07081488e-01],\n       [9.20310199e-01],\n       [4.13710326e-02],\n       [           nan],\n       [1.03716493e-01],\n       [7.34755844e-02],\n       [           nan],\n       [2.20237955e-01],\n       [1.05665326e-01],\n       [8.31335902e-01],\n       [8.58865976e-02],\n       [5.86883187e-01],\n       [5.65015912e-01],\n       [1.31194502e-01],\n       [           nan],\n       [           nan],\n       [7.42809176e-01],\n       [4.58195180e-01],\n       [9.40766275e-01],\n       [1.25255018e-01],\n       [           nan],\n       [5.97397745e-01],\n       [1.27368465e-01],\n       [9.84600961e-01],\n       [1.09754987e-01],\n       [1.05228111e-01],\n       [           nan],\n       [1.57015443e-01],\n       [5.75370908e-01],\n       [1.00016333e-02],\n       [1.82775617e-01],\n       [           nan],\n       [2.50863999e-01],\n       [9.82811749e-01],\n       [           nan],\n       [9.87014413e-01],\n       [1.23947792e-01],\n       [9.42739606e-01],\n       [1.23640731e-01],\n       [9.77898836e-01],\n       [           nan],\n       [1.16461083e-01],\n       [           nan],\n       [8.03801641e-02],\n       [1.09890454e-01],\n       [1.63848579e-01],\n       [9.86615658e-01],\n       [6.69486001e-02],\n       [           nan],\n       [5.59915662e-01],\n       [1.27443433e-01],\n       [4.21794653e-01],\n       [2.02899635e-01],\n       [9.44575727e-01],\n       [9.87117469e-01],\n       [9.76937652e-01],\n       [8.39905381e-01],\n       [1.94767356e-01],\n       [           nan],\n       [           nan],\n       [2.87374169e-01],\n       [9.28723097e-01],\n       [7.53220692e-02],\n       [9.14378226e-01],\n       [           nan],\n       [9.80310619e-01],\n       [1.27531126e-01],\n       [2.70148724e-01],\n       [1.14727214e-01],\n       [8.88547227e-02],\n       [           nan],\n       [           nan],\n       [1.00335553e-01],\n       [9.42763388e-01],\n       [1.23615615e-01],\n       [4.68724072e-02],\n       [1.23711213e-01],\n       [8.92151713e-01],\n       [5.94894171e-01],\n       [1.44878283e-01],\n       [           nan],\n       [           nan],\n       [           nan],\n       [           nan],\n       [1.40302449e-01],\n       [1.53510451e-01],\n       [           nan],\n       [9.83280659e-01],\n       [           nan],\n       [           nan],\n       [9.51092362e-01],\n       [1.20031506e-01],\n       [6.71328008e-02],\n       [7.38338530e-02],\n       [1.35117695e-01],\n       [5.33076108e-01],\n       [4.58161533e-01],\n       [           nan],\n       [8.51513803e-01],\n       [5.58103621e-01],\n       [7.82721713e-02],\n       [           nan],\n       [5.42007804e-01],\n       [           nan],\n       [           nan],\n       [           nan],\n       [6.72688901e-01],\n       [           nan],\n       [2.15299070e-01],\n       [8.04393962e-02],\n       [1.06477551e-01],\n       [9.77187455e-01],\n       [           nan],\n       [2.07522273e-01],\n       [9.71172750e-02],\n       [8.84645656e-02],\n       [           nan],\n       [1.15984656e-01],\n       [1.13979988e-01],\n       [           nan],\n       [9.49663043e-01],\n       [1.87514037e-01],\n       [7.27650762e-01],\n       [2.10650235e-01],\n       [1.73990786e-01],\n       [1.36252463e-01],\n       [1.94445804e-01],\n       [           nan],\n       [5.64990222e-01],\n       [9.34590101e-01],\n       [6.45430565e-01],\n       [2.59688169e-01],\n       [1.48637354e-01],\n       [1.03243724e-01],\n       [5.44634722e-02],\n       [1.06349610e-01],\n       [1.86559871e-01],\n       [1.26748115e-01],\n       [2.74111956e-01],\n       [9.64552701e-01],\n       [1.15609162e-01],\n       [8.49125385e-01],\n       [1.53926432e-01],\n       [7.91780204e-02],\n       [1.42507926e-01],\n       [8.10150981e-01],\n       [2.59621978e-01],\n       [           nan],\n       [6.74274981e-01],\n       [1.03286624e-01],\n       [2.87929326e-01],\n       [1.16384253e-01],\n       [2.14660600e-01],\n       [1.43294349e-01],\n       [           nan],\n       [1.71911746e-01],\n       [8.82888809e-02],\n       [           nan],\n       [9.98540759e-01],\n       [           nan],\n       [5.62603295e-01],\n       [1.26748115e-01],\n       [6.99544013e-01],\n       [1.31128371e-01],\n       [9.16736662e-01],\n       [9.52572644e-01],\n       [1.25255018e-01],\n       [1.54772133e-01],\n       [1.03831857e-01],\n       [7.18918502e-01],\n       [1.87319577e-01],\n       [9.56040800e-01],\n       [           nan],\n       [           nan],\n       [3.25712025e-01],\n       [7.87120487e-04],\n       [9.74139571e-01],\n       [9.16736662e-01],\n       [1.04080170e-01],\n       [9.97282267e-01],\n       [           nan],\n       [           nan],\n       [6.28560245e-01],\n       [9.56401587e-01],\n       [1.47009835e-01],\n       [7.77741894e-02],\n       [8.60121787e-01],\n       [1.62948623e-01],\n       [9.78512391e-02],\n       [9.83767152e-01],\n       [9.83546972e-01],\n       [1.42606899e-01],\n       [1.40478715e-01],\n       [1.49428844e-01],\n       [2.04088688e-02],\n       [           nan],\n       [1.54041439e-01],\n       [           nan],\n       [3.13555509e-01],\n       [           nan],\n       [8.74591172e-01],\n       [1.12985604e-01],\n       [8.07855800e-02],\n       [1.77302241e-01],\n       [3.90544236e-02],\n       [1.50811777e-01],\n       [9.64875042e-01],\n       [5.40160358e-01],\n       [9.11540464e-02],\n       [1.95086114e-02],\n       [9.97059643e-01],\n       [1.62155867e-01],\n       [9.91886675e-01],\n       [1.19980082e-01],\n       [1.32983997e-01],\n       [9.07467544e-01],\n       [6.54544011e-02],\n       [9.71211910e-01],\n       [2.43738577e-01],\n       [7.06630051e-01],\n       [2.53379762e-01],\n       [7.30424970e-02],\n       [3.35023761e-01],\n       [           nan],\n       [5.42722166e-01],\n       [           nan],\n       [9.92282569e-01],\n       [4.48056519e-01],\n       [           nan],\n       [9.03519750e-01],\n       [7.44565427e-02],\n       [           nan],\n       [           nan]], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"(predd > 0.5).astype(int)","execution_count":140,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in greater\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"},{"output_type":"execute_result","execution_count":140,"data":{"text/plain":"array([[0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"m1=pd.DataFrame(predd,columns=['Survived'])","execution_count":147,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1=pd.DataFrame(x1,columns=['PassengerId'])","execution_count":150,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sb=pd.concat([x1,m1],axis=1)","execution_count":151,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.head()","execution_count":152,"outputs":[{"output_type":"execute_result","execution_count":152,"data":{"text/plain":"   PassengerId  Survived\n0          892  0.120033\n1          893  0.147394\n2          894  0.101227\n3          895  0.104080\n4          896  0.167398","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>0.120033</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>0.147394</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>0.101227</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>0.104080</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>0.167398</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.to_csv('submissionbest.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}